---
title: Deployment best practices
subtitle: Best practices and workflows for deploying with managed federation
description: Best practices and workflows for deploying with managed federation
---

When rolling out changes to a [subgraph](../building-supergraphs/subgraphs-overview/), we recommend the following workflow:

1. Confirm the backward compatibility of each change by [running `rover subgraph check`](/rover/commands/subgraphs/#validating-subgraph-schema-changes) in your CI pipeline.
2. Merge backward compatible changes that successfully pass schema checks.
3. Deploy changes to the subgraph in your infrastructure.
4. Wait until all replicas finish deploying.
5. Run [`rover subgraph publish`](/rover/commands/subgraphs/#publishing-a-subgraph-schema-to-graphos) to update your managed federation configuration:

```bash
rover subgraph publish my-supergraph@my-variant \
  --schema ./accounts/schema.graphql \
  --name accounts \
  --routing-url https://my-running-subgraph.com/api
```

## Pushing configuration updates safely

Whenever possible, you should update your subgraph configuration in a way that is backward compatible to avoid downtime. As suggested above, the best way to do this is to run `rover subgraph check` before updating. You should also generally seek to minimize the number of [breaking changes](/graphos/delivery/schema-checks/#potentially-breaking-changes) you make to your schemas.

Additionally, **call `rover subgraph publish` for a subgraph only after all replicas of that subgraph are deployed**. This ensures that resolvers are in place for all operations that are executable against your graph, and operations can't attempt to access fields that do not yet exist.

In the rare case where a configuration change is _not_ backward compatible with your router's query planner, you should update your registered subgraph schemas _before_ you deploy your updated code.

You should also perform configuration updates that affect query planning prior to (and separately from) other changes. This helps avoid a scenario where the query planner generates queries that fail validation in downstream services or violate your resolvers.

Examples of this include:

- Modifying `@key`, `@requires`, or `@provides` directives
- Removing a type implementation from an interface

In general, always exercise caution when pushing configuration changes that affect your router's query planner, and consider how those changes will affect your other subgraphs.

### Example scenario

Let's say we define a `Channel` interface in one subgraph, and we define types that _implement_ `Channel` in two other subgraphs:

```graphql
# channel subgraph
interface Channel @key(fields: "id") {
  id: ID!
}

# web subgraph
type WebChannel implements Channel @key(fields: "id") {
  id: ID!
  webHook: String!
}

# email subgraph
type EmailChannel implements Channel @key(fields: "id") {
  id: ID!
  emailAddress: String!
}
```

To safely remove the `EmailChannel` type from your supergraph schema:

1. Perform a `rover subgraph publish` of the `email` subgraph that removes the `EmailChannel` type from its schema.
2. Deploy a new version of the subgraph that removes the `EmailChannel` type.

The first step causes the query planner to stop sending fragments `...on EmailChannel`, which would fail validation if sent to a subgraph that isn't aware of the type.

If you want to _keep_ `EmailType` but remove it from the `Channel` interface, the process is similar. Instead of removing the `EmailChannel` type altogether, only remove the `implements Channel` addendum to the type definition. This is because the query planner expands queries to interfaces or unions into fragments on their implementing types.

For example, a query such as...

```graphql
query FindChannel($id: ID!) {
  channel(id: $id) {
    id
  }
}
```

...generates two queries, one to each subgraph, like so:

```graphql
# Generated by the query planner

# To email subgraph
query {
  _entities(...) {
    ...on EmailChannel {
      id
    }
  }
}

# To web subgraph
query {
  _entities(...) {
    ...on WebChannel {
      id
    }
  }
}
```

Currently, the router expands all interfaces into implementing types.

## Removing a subgraph

To "de-register" a subgraph with Apollo, call `rover subgraph delete`:

> **This action cannot be reversed!**

```
rover subgraph delete my-supergraph@my-variant --name accounts
```

The next time it starts up or polls, your router obtains an updated configuration that reflects the removed subgraph.

## Using variants to control rollout

Your deployment model may require deploying a specific version of your graph, for example for blue-green deployments or canary deployments. With managed federation, you can control which version of your graph a fleet of routers are using. In the majority of cases, rolling over all of your router instances to a new schema version is safe, assuming you've used [schema checks](./federated-schema-checks/) to confirm that your changes are backward compatible.

However, changes at the router level might involve a variety of different updates, such as [migrating entities](../entities-advanced/#migrating-entities-and-fields) from one subgraph to another. If your infrastructure requires a more advanced deployment process, you can use [graph variants](/graphos/graphs/#variants) to manage different fleets of routers running with different configurations.

Specifically, for deployments that run in different production environments, such as blue-green deployments, you can configure your environments to refer to a single graph variant by pinning the supergraph schema of each environment to your routers at deployment time. Using a single variant between different production environments enables Studio to get usage reports and do analysis on the combined production traffic of all environments.

### Example blue-green deployment

A blue-green deployment strategy uses two environments: one environment (blue) serves the schema variant for live traffic, while the other environment (green) uses a variant for a new release that's under development. When the new release is ready, traffic is migrated from the blue to the green environment. This cycle repeats with each new release.  

As an example, follow these steps to deploy with a supergraph schema of a new release (green) environment:

1. Publish all subgraphs of a release at once by using the Platform API [`publishSubgraphs`](https://studio.apollographql.com/graph/apollo-platform/variant/main/schema/reference/objects/GraphMutation#publishSubgraphs).

    ```graphql
    ## Publish multiple subgraphs together in a batch
    ## and retrieve the associated launch, along with any downstream launches.
    mutation PublishSubgraphsMutation(
      $graphId: ID!
      $graphVariant: String!
      $revision: String!
      $subgraphInputs: [PublishSubgraphsSubgraphInput!]!
    ) {
      graph(id: $graphId) {
        publishSubgraphs( #highlight-line
          graphVariant: $graphVariant
          revision: $revision
          subgraphInputs: $subgraphInputs
        ) {
          launch {
            id
            downstreamLaunches {
              id
            }
          }
        }
      }
    }
    ```

  This initiates a launch as well as downstream launches, and it returns the IDs for the launch and any downstream launches. 

1. Poll for the completed launch and any downstream launches. 

    ```graphql
    ## Poll for the status of any individual launch by ID
    query PollLaunchStatusQuery($graphId: ID!, $name: String!, $launchId: ID!) {
      graph(id: $graphId) {
        variant(name: $name) {
          launch(id: $launchId) {
            status
          }
        }
      }
    }

    ```

1. After the launch and downstream launches have completed, retrieve the supergraph schema of the launch.

    ```graphql
    ## Fetch the supergraph SDL by launch ID.
    query FetchSupergraphSDLQuery($graphId: ID!, $launchId: String!) {
      graph(id: $graphId) {
        variant(name: $name) {
          launch(id: $launchId) {
            build {
              result {
                ... on BuildSuccess {
                  coreSchema {
                    coreDocument
                  }
                }
              }
            }
          }
        }
      }
    }

    ```

1. Deploy your routers with the [`-s` or `--supergraph` option](/router/configuration/overview/#-s----supergraph) to specify the supergraph schema.

    * For an example using the option in a `docker run` command, see [Specifying the supergraph](/router/containerization/docker/#specifying-the-supergraph).

### Example canary deployment

A canary deployment applies graph updates to a small subset of your deployment environment before rolling it out for your entire environment.

To configure a canary deployment, you might maintain two production graph variants in Apollo Studio, one named `prod` and the other named `prod-canary`. To deploy a change to a subgraph named `launches`, you might perform the following steps:

1. Check the changes in `launches` against both `prod` and `prod-canary`:
   ```shell
   rover subgraph check my-supergraph@prod --name launches --schema ./launches/schema.graphql
   rover subgraph check my-supergraph@prod-canary --name launches --schema ./launches/schema.graphql
   ```
2. Deploy your changes to the `launches` subgraph in your production environment, _without_ running `rover subgraph publish`.
    * _This ensures that your production router's configuration is not updated yet._
3. Update your `prod-canary` variant's registered schema, by running:
    ```
    rover subgraph publish my-supergraph@prod-canary --name launches --schema ./launches/schema.graphql
    ```
    * _If composition fails due to intermediate changes to the canary graph, the canary router's configuration will not be updated._
4. Wait for health checks to pass against the canary and confirm that operation metrics appear as expected.
5. After the canary is stable, roll out the changes to your production routers:
    ```
    rover subgraph publish my-supergraph@prod --name=launches --schema ./launches/schema.graphql
    ```

If your canary variant [reports metrics to GraphOS](/graphos/metrics/), you can use [Apollo Studio](https://studio.apollographql.com?referrer=docs-content) to verify a canary's performance before rolling out changes to the rest of the graph. You can also use variants to support a variety of other advanced deployment workflows, such as blue/green deployments.

## Modifying query-planning logic

Treat migrations of your query-planning logic similarly to how you treat database migrations. Carefully consider the effects on downstream services as the query planner changes, and plan for "double reading" as appropriate.

Consider the following example of a `Products` subgraph and a `Reviews` subgraph:

```graphql
# Products subgraph

type Product @key(fields: "upc") {
  upc: ID!
  nameLowerCase: String!
}

# Reviews subgraph

type Product @key(fields: "upc") {
  upc: ID!
  reviews: [Review]! @requires(fields: "nameLowercase")
  nameLowercase: String! @external
}
```

Let's say we want to deprecate the `nameLowercase` field and replace it with the `name` field, like so:

```graphql
# Products subgraph

type Product @key(fields: "upc") {
  upc: ID!
  nameLowerCase: String! @deprecated
  name: String!
}

# Reviews subgraph

type Product @key(fields: "upc") {
  upc: ID!
  nameLowercase: String! @external
  name: String! @external
  reviews: [Review]! @requires(fields: "name")
}
```

To perform this migration in-place:

1. Modify the `Products` subgraph to add the new field. (As usual, first deploy all replicas, then use `rover subgraph publish` to push the new subgraph schema.)
2. Deploy a new version of the `Reviews` subgraph with a resolver that accepts _either_ `nameLowercase` or `name` in the source object.
3. Modify the Reviews subgraph's schema in the registry so that it `@requires(fields: "name")`.
4. Deploy a new version of the `Reviews` subgraph with a resolver that _only_ accepts the `name` in its source object.

Alternatively, you can perform this operation with an atomic migration at the subgraph level, by modifying the subgraph's URL:

1. Modify the `Products` subgraph to add the `name` field (as usual, first deploy all replicas, then use `rover subgraph publish` to push the new subgraph schema).
2. Deploy a new set of `Reviews` replicas to a new URL that reads from `name`.
3. Register the `Reviews` subgraph with the new URL and the schema changes above.

With this atomic strategy, the query planner resolves all outstanding requests to the old subgraph URL that relied on `nameLowercase` with the old query-planning configuration, which `@requires` the `nameLowercase` field. All _new_ requests are made to the new subgraph URL using the new query-planning configuration, which `@requires` the `name` field.

## Reliability and security

Your router fetches its configuration by polling Apollo Uplink, an Apollo-hosted endpoint specifically for serving supergraph configs. In the event that your updated config is inaccessible due to an outage in Uplink, your router continues to serve its most recently fetched configuration.

If you restart a router instance or spin up a _new_ instance during an Uplink outage, that instance can't fetch its configuration until Apollo resolves the outage.

## The `subgraph publish` lifecycle

Whenever you call `rover subgraph publish` for a particular subgraph, it both updates that subgraph's registered schema and updates the router's managed configuration.

Because your graph is dynamically changing and multiple subgraphs might be updated simultaneously, it's possible for changes to cause composition errors, even if `rover subgraph check` was successful. For this reason, updating a subgraph re-triggers composition in the cloud, ensuring that all subgraphs still compose to form a complete supergraph before updating the configuration. The workflow behind the scenes can be summed up as follows:

1. The subgraph schema is uploaded to Apollo and indexed.
2. The subgraph is updated in the registry to use its new schema.
3. All subgraphs are composed in the cloud to produce a new supergraph schema.
4. If composition fails, the command exits and emits errors.
5. If composition succeeds, Apollo Uplink begins serving the updated supergraph schema.

On the other side of the equation sits the router. The router regularly polls Apollo Uplink for changes to its configuration. The lifecycle of dynamic configuration updates is as follows:

1. The router polls for updates to its configuration.
2. On update, the router downloads the updated configuration, including the new supergraph schema.
3. The router uses the new supergraph schema to update its query planning logic.
4. The router continues to resolve in-flight requests with the previous configuration, while using the updated configuration for all new requests.
